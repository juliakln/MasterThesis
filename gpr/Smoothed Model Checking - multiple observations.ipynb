{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: estimate satisfaction function of a formula from instances of its satisfaction on individual runs at discrete parameter values\n",
    "\n",
    "We use Gaussian Process Classification to estimate the satisfaction function.\n",
    "Problem: the exact computation of the posterior probability is not possible --> use approximation from **Expectation Propagation (EP)** approach (high accuracy + computational efficiency) \n",
    "\n",
    "EP computes a Gaussian approximation to probabilistic models of the form\n",
    "$ p(x|y) = p_0(x) \\prod_i t_i (y_i,x_i)$   \n",
    "\n",
    "$p_0(x)$ is a multivariate Gaussian distribution coupling all $x_i$ variables (*site variables*), $t_i$ can be general univariate distributions. Those models are calles *latent Gaussian models*: p0 represents prior distribution, with ti representing non-Gaussian observation likelihoods.  \n",
    "EP approximation: likelihood terms replaced by univariate Gaussian terms $ q(x|y) = p_0(x) \\prod_i \\tilde{t}_i (y_i,x_i) $  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "import math\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.special import erf \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel\n",
    "\n",
    "We place a GP prior over the latent function $f(x)$ and then squash it to obtain the prior on $\\pi(x) = p(y=1|x) = \\sigma(f(x))$.  \n",
    "We use the squared exponential covariance function $k(x,x')= \\sigma^2 exp(- \\frac{(x-x')^2}{2\\lambda^2}) $.  \n",
    "Squash it with: probit transformation (cdf of normal distribution, $\\Phi(z) = \\int^z_{-\\inf} \\mathcal{N}(x|0,1)dx$).\n",
    "\n",
    "We don't observe values of f itself, only inputs X and class labels y. We are only interested in $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_rbf(X1, X2, param):\n",
    "    \"\"\"\n",
    "    Squared exponential kernel.\n",
    "\n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "        param: Kernel parameters (2)\n",
    "\n",
    "    Returns:\n",
    "        (m x n) matrix\n",
    "    \"\"\"\n",
    "\n",
    "    sqdist = np.sum(X1 ** 2, 1).reshape(-1, 1) + np.sum(X2 ** 2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return param[1] ** 2 * np.exp(-0.5 / param[0] ** 2 * sqdist)\n",
    "\n",
    "\n",
    "# default hyperparameters of kernel (could optimize it, but here just default values)\n",
    "params = [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "2 steps:  \n",
    "- compute distribution of latent variable corresponding to a test case: $p(f_*|X,y,x_*) = \\int p(f_*|X,x_*,f) p(f|X,y) df$, where $p(f|X,y) = p(y|f)p(f|X)/p(y|X)$ is the posterior over the latent variables      \n",
    "\n",
    "   \n",
    "     \n",
    "- use this distribution over latent $f_*$ to produce probabilistic prediction: $\\overline{x}_* = p(y_*=1|X,y,x_*) = \\int \\sigma(f_*)p(f_*|X,y,x_*)df$   \n",
    "\n",
    "\n",
    "Non-Gaussian likelihood makes integral intractable $\\rightarrow$ approximate with Gaussian posterior $\\Rightarrow$ Expectation Propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Propagation\n",
    "\n",
    "Posterior given by Bayes rule as product of normalization term, prior and likelihood: $p(f|X,y) = \\frac{1}{Z} p(f|X) \\prod p(y_i|f_i)$  \n",
    "Prior $p(f|X)$ is Gaussian  \n",
    "Likelihood factorizes over training cases  \n",
    "Normalization term = marginal likelihood: $Z = p(y|X) = \\int p(f|X) \\prod p(y_i|f_i) df$  \n",
    "\n",
    "Probit likelihood for binary classification: $p(y_i|f_i) = \\Phi(f_iy_i)$  \n",
    "$\\rightarrow$ approximate by local likelihood approximation (unnormalized Gaussian function in latent variable $f_i$): $p(y_i|f_i) \\approx t_i ( f_i|\\tilde{Z}_i,\\tilde{\\mu}_i,\\tilde{\\sigma}_i^2) = \\tilde{Z}_i \\mathcal{N}(f_i|\\tilde{\\mu}_i,\\tilde{\\sigma}_i^2)$  \n",
    "\n",
    "Product of independent local likelihoods $t_i$: $\\prod t_i(f_i| \\tilde{Z}_i,\\tilde{\\mu}_i,\\tilde{\\sigma}_i^2) = \\mathcal{N}(\\tilde{\\mu},\\tilde{\\Sigma}) \\prod \\tilde{Z}_i$, where $\\tilde{\\mu}$ is vector of $\\tilde{\\mu}_i$ and $\\tilde{\\Sigma}$ is diagonal with $\\tilde{\\Sigma}_{ii} = \\tilde{\\sigma}^2_i$.  \n",
    "\n",
    "Approximate posterior $p(f|X,y)$ by: $q(f|X,y) = \\frac{1}{Z_{EP}} p(f|X) \\prod t_i(f_i| \\tilde{Z}_i,\\tilde{\\mu}_i,\\tilde{\\sigma}_i^2) = \\mathcal{N}(\\mu,\\Sigma)$ [3.53] with $Z_{EP}=q(y|X)$ (approximation to normalizing term Z from previous eq.), $\\mu=\\Sigma\\tilde{\\Sigma}^{-1}\\tilde{\\mu}$ and $\\Sigma=(K^{-1}+\\tilde{\\Sigma}^{-1})^{-1}$  \n",
    "\n",
    "### How do we choose parameters of local approximating distributions $t_i$?  \n",
    "Update individual $t_i$ approximations sequentially. Iterate these 4 steps until convergence:  \n",
    "1. start from some current approximate posterior, from which we leave out current $t_i$ $\\rightarrow$ marginal cavity distribution  \n",
    "2. Combine cavity distribution with exact likelihood $p(y_i|f_i)$ to get desired (non-Gaussian) marginal  \n",
    "3. Choose Gaussian approximation to marginal  \n",
    "4. Compute the $t_i$ which makes posterior have the desired marginal from step 3   \n",
    "\n",
    "Approximate posterior for $f_i$ contains 3 kinds of terms:  \n",
    "1. Prior $p(f|X)$  \n",
    "2. Local approximate likelihoods $t_j$ for all cases $j\\neq i$  \n",
    "3. Exact likelihood for case i, $p(y_i|f_i) = \\Phi(y_if_i)$   \n",
    "\n",
    "Goal: combine these sources of information and choose parameters of $t_i$ such that the marginal posterior is as accurate as possible. \n",
    "\n",
    "1. Combine prior and local likelihood approximations into cavity distribution: $q_{-i}(f_i) \\propto \\int p(f|X) \\prod_{j\\neq i} t_j (f_j|\\tilde{Z}_j, \\tilde{\\mu}_j, \\tilde{\\sigma}_j^2) df_j$  \n",
    "Either by explicitly multiplying out the terms, or by removing approximate likelihood i from the approximate posterior in 3.53.\n",
    "2. Combine this with exact likelihood for case i. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Initialization, perform statistical model checking\n",
    "\n",
    "- Set parameters for smoothed model checking  \n",
    "- Invent parameters: assume 10 trajectories per input point and statistically estimate outputs (number of runs satisfying property / total number of runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual data (number of runs satisfying property):  [[7. 5. 9. 1. 0. 3. 4. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# parameters for smoothed model checking\n",
    "simulation_runs = scale = 10  # number of trajectories per input point\n",
    "paramValueSet = np.array([2,3,5,7,8,10,11,12,14]).reshape(-1,1) # uncertain input parameter that is varied = population size\n",
    "datapoints = len(paramValueSet)  # number of input points\n",
    "paramValueOutputs = np.array([0.7,0.5,0.9,0.1,0,0.3,0.4,0.1,0.2]).reshape(-1,1) # statistical outputs of satisfaction \n",
    "data = trainingSet = [paramValueSet, paramValueOutputs] # set as training set for GP\n",
    "\n",
    "# correction for covariance matrix calculation\n",
    "correction = 1e-4\n",
    "\n",
    "# plot training data\n",
    "plt.scatter(paramValueSet, paramValueOutputs, marker='o', c='blue')\n",
    "plt.title('Training dataset with 10 trajectories per input point')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.yticks([0, 1])\n",
    "plt.show()\n",
    "\n",
    "print(\"Actual data (number of runs satisfying property): \", (paramValueOutputs * scale).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Perform smoothed model checking (SMC 65)\n",
    "\n",
    "getAnalyticApproximation(data, parameters, options)  \n",
    "-> optimize hyperparameters for rbf kernel or take default ones  \n",
    "-> doTraining = probit gp regression  \n",
    "   -> expectationPropagation\n",
    "   \n",
    "then return performSmoothedModelChecking(approx, parameters, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for EP\n",
    "\n",
    "! Attention: computation of cholesky(A) may fail when A is not PD (for large amplitudes) --> that's why I added another +I for computational stability  \n",
    "\n",
    "\n",
    "Term:  \n",
    "$\\tilde{v} = ... $  \n",
    "$\\tilde{\\tau} = 1/\\tilde{\\sigma}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_moments(Term, gauss_LC, gauss_LC_t):\n",
    "    \"\"\"\n",
    "    Computes marginal moments\n",
    "    \n",
    "    Args:\n",
    "        Term: v_tilde, tau_tilde (datapoints, 2)\n",
    "        gauss_LC:\n",
    "        gauss_LC_t: \n",
    "        \n",
    "    Returns:\n",
    "        logZappx\n",
    "        gauss_m (datapoints,1)\n",
    "        gauss_diagV: diagonal of sigma^2 (datapoints,1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # A = LC' * (tau_tilde * gauss_LC) + I \n",
    "    # Q: Why add 1 as correction? Not too much? \n",
    "    tmp = np.multiply(Term[:,1], gauss_LC)\n",
    "    A = np.matmul(gauss_LC_t, tmp) + np.eye(datapoints)\n",
    "\n",
    "    gauss_L = np.linalg.cholesky(A + np.eye(datapoints)).T  \n",
    "#    gauss_L = np.linalg.cholesky(A).T  \n",
    "\n",
    "    # W = L\\LC'\n",
    "    gauss_W = np.linalg.solve(gauss_L, gauss_LC_t)\n",
    "    gauss_diagV = np.diagonal(np.matmul(gauss_W.T, gauss_W)).reshape(-1,1)\n",
    "    \n",
    "    # m = W'*(W * v_tilde)\n",
    "    tmp = np.matmul(gauss_W, Term[:,0])\n",
    "    gauss_m = np.matmul(gauss_W.T, tmp).reshape(-1,1)\n",
    "\n",
    "    # logdet = -2*sum(log(diag(L))) + 2*sum(log(diag(LC)))\n",
    "    logdet = -2*np.sum(np.log(np.diagonal(gauss_L))) # + 2*np.sum(np.log(np.diag(gauss_LC))) (das ist schon logdet_LC)\n",
    "    logdet += logdet_LC\n",
    "\n",
    "    # logZappx = 1/2(m' * v_tilde + logdet)\n",
    "    logZappx = 0.5 * (np.dot(gauss_m.T, Term[:,0]) + logdet)\n",
    "\n",
    "    return logZappx, gauss_m, gauss_diagV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Gauss–Hermite_quadrature  \n",
    "\n",
    "Gauss-Hermite quadrature is a form of Gaussian quadrature for approximating the value of integrals of the following kind:  \n",
    "\n",
    "$ \\int_{-\\infty}^{+\\infty} e^{-x^2} f(x) dx \\approx \\sum_{i=1}^n w_i f(x_i)  $  \n",
    "\n",
    "where $n$ is the number of sample points used (here: nodes). The $x_i$ are the roots of the physicists' version of the Hermite polynomial $H_n(x)$ and the associated weights $w_i$ are given by:  \n",
    "\n",
    "$ w_i = \\frac{ 2^{n-1} n! \\sqrt{\\pi}}{n^2 [H_{n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausshermite(nodes):\n",
    "    \"\"\"\n",
    "    Gauss-Hermite \n",
    "    https://indico.frib.msu.edu/event/15/attachments/40/157/Gaussian_Quadrature_Numerical_Recipes.pdf\n",
    "    \n",
    "    Approximate integral of a formula by the sum of its functional values at some points\n",
    "    \n",
    "    Args:\n",
    "        nodes: number of Gauss-Hermite nodes (1)\n",
    "        \n",
    "    Returns:\n",
    "        x0: abscissas\n",
    "        w0: weights\n",
    "    \"\"\"\n",
    "\n",
    "    x0 = np.zeros((nodes, 1))\n",
    "    w0 = np.zeros((nodes, 1))\n",
    "    m = int((nodes+1)/2)\n",
    "    z,pp,p1,p2,p3 = 0,0,0,0,0\n",
    "    \n",
    "    for i in range(m):\n",
    "        if i==0:\n",
    "            z = np.sqrt(2*nodes+1) - 1.85575 * ((2*nodes+1)**(-0.166667))\n",
    "        elif i==1:\n",
    "            z = z - 1.14 * (nodes**0.426) / z\n",
    "        elif i==2:\n",
    "            z = 1.86 * z - 0.86 * x0[0]\n",
    "        elif i==3:\n",
    "            z = 1.91 * z - 0.91 * x0[1]\n",
    "        else:\n",
    "            z = 2.0 * z - x0[i - 2]\n",
    "\n",
    "        for its in range(10):\n",
    "            p1 = 1/np.sqrt(np.sqrt(np.pi))\n",
    "            p2 = 0\n",
    "            for j in range(1,nodes+1):\n",
    "                p3=p2\n",
    "                p2=p1\n",
    "                a = z*np.sqrt(2/j)*p2\n",
    "                b = np.sqrt((j-1)/j)*p3\n",
    "                p1=a-b\n",
    "            pp=np.sqrt(2*nodes)*p2\n",
    "            z1=z\n",
    "            z=z1-p1/pp\n",
    "            if np.abs(z-z1)<2.2204e-16:\n",
    "                break\n",
    "\n",
    "        x0[i] = z\n",
    "        x0[nodes-1-i] = -z\n",
    "        w0[i] = 2/(pp*pp)\n",
    "        w0[nodes-1-i] = w0[i]\n",
    "\n",
    "    w0 = np.divide(w0, np.sqrt(np.pi))\n",
    "    x0 = np.multiply(x0, np.sqrt(2))\n",
    "    x0 = np.sort(x0, axis=None).reshape(-1,1)\n",
    "    \n",
    "    return x0, w0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute new posterior marginal moments $\\hat{\\mu}$ and $\\hat{\\sigma}^2$. Find new Gaussian marginal which best approximates the product of the cavity distribution and the exact likelihood $\\hat{q}(f_i) \\overset{\\Delta}{=} \\hat{Z}_i ~ \\mathcal{N}(\\hat{\\mu}_i, \\hat{\\sigma}^2_i) \\simeq q_{-i} (f_i) p (y_i | f_i) $.  \n",
    "Well known that distribution $q(x)$ which minimizes KL$(p(x)||q(x))$ is the onw whose first and second moments match that of $p(x)$. $\\hat{q}(f_i)$ is unnormalized, so choose additionally the condition that zero-th moments (normalizing constants) should match when choosing the parameters of $\\hat{q}(f_i)$ to match the right-hand side of above equation.  \n",
    "Derivation of moments is complicated (see Appendix 3.9). Desired posterior marginal moments are:  \n",
    "\n",
    "Literature (Rasmussen 3.58):  \n",
    "\n",
    "$ \\hat{Z} = \\Phi (z)$  \n",
    "\n",
    "$ \\hat{\\mu} = \\mu_{-i} + \\frac{y ~ \\sigma^2_{-i} \\mathcal{N}(z)} { \\Phi(z) \\sqrt{1 + \\sigma^2_{-i}}} $  \n",
    "\n",
    "$ \\hat{\\sigma}^2 = \\sigma^2_{-i} - \\frac{ \\sigma^4_{-i} \\mathcal{N}(z)} {(1 + \\sigma^2_{-i}) \\Phi (z) } (z + \\frac{\\mathcal{N}(z)} {\\Phi (z)} ) $,  \n",
    "\n",
    "\n",
    "where $z = \\frac{y ~ \\mu_{-i}} { \\sqrt{1 + \\sigma^2_{-i}} } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussHermiteNQ(FuncPar_p, FuncPar_q, cav_m, cav_v, xGH, logwGH):\n",
    "    \"\"\"\n",
    "    Gauss-Hermite numerical quadrature for Gaussian integration and moment computation\n",
    "    \n",
    "    Args:\n",
    "        FuncPar_p: number of runs satisfying property for each parameter value (input) (datapoints,1)\n",
    "        FuncPar_q: number of runs not satisfying property (datapoints,1)\n",
    "        cav_m: cavity mean mu_-i\n",
    "        cav_v: cavity variance sigma^2_-i\n",
    "        xGH: abscissas (Gauss-Hermite)\n",
    "        logwGH: weights (Gauss-Hermite)\n",
    "        \n",
    "    Returns:\n",
    "        logZ\n",
    "        Cumul: mu_hat, sigma^2_hat (datapoints,2)\n",
    "    \"\"\"\n",
    "    \n",
    "    Nnodes = len(xGH)\n",
    "    \n",
    "    # sigma_-i\n",
    "    stdv = np.sqrt(cav_v).reshape(-1,1)\n",
    "\n",
    "    # \n",
    "    Y = np.matmul(stdv, xGH.reshape(1,-1)) + numpy.matlib.repmat(cav_m, 1, Nnodes)\n",
    "        \n",
    "    G = np.array(logprobitpow(Y, FuncPar_p, FuncPar_q) + numpy.matlib.repmat(logwGH.T, datapoints, 1))\n",
    "    \n",
    "    # maximum of each row (input value) over all 96 nodes\n",
    "    maxG = G.max(axis=1).reshape(-1,1)\n",
    "    # subtract maximum value\n",
    "    G = G - np.matlib.repmat(maxG, 1, 96)\n",
    "    # exponential value\n",
    "    expG = np.exp(G)\n",
    "    # denominator (row sum)\n",
    "    denominator = expG.sum(axis=1).reshape(-1,1)\n",
    "    logdenominator = np.log(denominator)\n",
    "    logZ = maxG + logdenominator\n",
    "    \n",
    "    Cumul = np.zeros((len(FuncPar_p), 2))\n",
    "\n",
    "\n",
    "    # deltam = stdv * (expG * xGH) / denominator\n",
    "    deltam = np.divide(np.multiply(stdv, np.matmul(expG, xGH)), denominator)\n",
    "\n",
    "    # mu_hat = mu_-i + deltam    \n",
    "    Cumul[:,0] = (cav_m + deltam).reshape(-1)\n",
    "    \n",
    "    xGH2 = xGH**2\n",
    "    deltam2 = deltam**2\n",
    "\n",
    "    # sigma^2_hat = \n",
    "    Cumul[:,1] = (np.divide(np.multiply(cav_v, np.matmul(expG, xGH2)), denominator) - deltam2).reshape(-1)\n",
    "        \n",
    "    return logZ, Cumul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cavity parameters $\\mu_{-i}$ and $\\sigma^2_{-i}$   \n",
    "\n",
    "Literature (Rasmussen) 3.56:   \n",
    "\n",
    "\n",
    "$cav_m = \\mu_{-i} = \\sigma_{-i}^2 \\cdot ( \\frac{\\mu}{\\sigma^2} - \\frac{\\tilde{\\mu}}{\\tilde{\\sigma}^2} )$   \n",
    "\n",
    "\n",
    "\n",
    "$cav_v =  \\sigma_{-i}^2 = \\frac{1}{\\sigma^{-2} - \\tilde{\\sigma}^{-2}}  $   \n",
    "\n",
    "\n",
    "Here: $diagV = \\sigma^2$ and $\\tilde{\\tau} = \\tilde{\\sigma}^{-2}$ and $\\tilde{v} = \\tilde{\\mu} \\tilde{\\sigma}^{-2}$; transforming the equations shows that they're equal:      \n",
    "\n",
    "$cav_m = \\frac{m + (- \\tilde{v} \\cdot diagV) }{1 + (- \\tilde{\\tau} \\cdot diagV)} = \\frac{ \\mu - \\tilde{\\sigma}^{-2} \\tilde{\\mu} \\sigma^2}{1 - \\tilde{\\sigma}^{-2} \\sigma^2} $   \n",
    "\n",
    "\n",
    "$cav_{diagV} = \\frac{diagV}{1 + (- \\tilde{\\tau} * diagV)} = \\frac{1}{diagV^{-1} - \\tilde{\\tau}} = \\frac{1}{\\sigma^{-2} - \\tilde{\\sigma}^{-2}}$   \n",
    "\n",
    "\n",
    "The cavity distribution combines the prior and the local likelihood approximations; afterwards, it is combined with the exact likelihood for case $i$. To do so, we remove the approximate likelihood $i$ from the approximate posterior by dividing the marginal with the approximate term $t_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cavities(gauss_diagV, gauss_m, Term):\n",
    "    \"\"\"\n",
    "    Compute cavities\n",
    "    \n",
    "    Args:\n",
    "        gauss_diagV: sigma^2\n",
    "        gauss_m: mu\n",
    "        Term: v_tilde, tau_tilde (datapoints,2)\n",
    "        \n",
    "    Returns:\n",
    "        cav_m: cavity mean mu_-i\n",
    "        cav_diagV: cavity variance sigma^2_-i\n",
    "    \"\"\"\n",
    "    \n",
    "    # s = 1 / (1 + -tau_tilde * diagV)\n",
    "    s = np.divide(1, (1 + np.multiply(-Term[:,1].reshape(-1,1), gauss_diagV)))\n",
    "\n",
    "    # cav_diagV = s * diagV\n",
    "    cav_diagV = np.multiply(s, gauss_diagV)\n",
    "    \n",
    "    # m = s * (m + (-v_tilde * diagV))\n",
    "    cav_m = np.multiply(s, (gauss_m + np.multiply(-Term[:,0].reshape(-1,1), gauss_diagV)))\n",
    "    \n",
    "    return cav_m, cav_diagV\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update site parameters $\\tilde{v}$ and $\\tilde{\\tau}$ with new posterior marginal moments $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ 3.59, 3.58  \n",
    "\n",
    "$\\tilde{v} = \\hat{\\sigma}_i^{-2} \\hat{\\mu}_i - \\sigma_{-i}^{-2} \\mu_{-i}$  \n",
    "\n",
    "$\\tilde{\\tau} = 1 / \\hat{\\sigma}_i^2 - 1 / \\sigma_{-i}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ep_update(cav_diagV, cav_m, Term, eps_damp, gauss_LikPar_p,\n",
    "              gauss_LikPar_q, gauss_xGauss, gauss_logwGauss):\n",
    "    \"\"\"\n",
    "    Update site parameters\n",
    "    \n",
    "    Args:\n",
    "        cav_diagV: cavity variance sigma^2_-i\n",
    "        cav_m: cavity mean mu_-i\n",
    "        Term: v_tilde, tau_tilde (datapoints,2)\n",
    "        eps_damp: 0.5\n",
    "        gauss_LikPar_p: number of runs satisfying property for each parameter (datapoints,1)\n",
    "        gauss_LikPar_q: number of runs not satisfying property (datapoints,1)\n",
    "        gauss_xGauss: abscissas of Gauss-Hermite\n",
    "        gauss_logwGauss: weights of Gauss-Hermite\n",
    "        \n",
    "    Returns:\n",
    "        TermNew: updated v_tilde, tau_tilde (datapoints,2)\n",
    "        logZterms:\n",
    "        logZ:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cumul = [mu_hat, sigma^2_hat]\n",
    "    logZ, Cumul = GaussHermiteNQ(gauss_LikPar_p, gauss_LikPar_q, cav_m, cav_diagV, gauss_xGauss, gauss_logwGauss)\n",
    "        \n",
    "    m2 = cav_m**2\n",
    "    logV = np.log(cav_diagV)\n",
    "    \n",
    "    cumul1 = (Cumul[:,0]**2).reshape(-1,1)\n",
    "    cumul2 = (np.log(Cumul[:,1])).reshape(-1,1)    \n",
    "    \n",
    "    logZterms = logZ + np.multiply(np.divide(m2, cav_diagV) + logV - \n",
    "                                   (np.divide(cumul1, Cumul[:,1].reshape(-1,1)) + cumul2), 1/2)\n",
    "        \n",
    "    c1 = np.divide(Cumul[:,0].reshape(-1,1), Cumul[:,1].reshape(-1,1)) - (np.divide(cav_m, cav_diagV))\n",
    "    c2 = np.divide(np.ones((datapoints,1)), Cumul[:,1].reshape(-1,1)) - (np.divide(np.ones((datapoints,1)), cav_diagV))\n",
    "        \n",
    "    TermNew = np.concatenate((c1, c2), axis=1)\n",
    "\n",
    "    TermNew = np.multiply(Term, (1 - eps_damp)) + np.multiply(TermNew, eps_damp)\n",
    "\n",
    "    return TermNew, logZterms, logZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix 3.9  \n",
    "\n",
    "$\\Phi(f(x)) = \\frac{1}{2} (1 + erf( x / \\sqrt{2} )) $  \n",
    "\n",
    "Here multiplication with true observation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobitpow(X, p, q):\n",
    "    \"\"\"\n",
    "    Compute ncdflogbc for matrices -> log of standard normal cdf by 10th order Taylor expansion in the negative domain\n",
    "    log likelihood evaluations for various probit power likelihoods\n",
    "    \n",
    "    Args:\n",
    "        X: matrix (datapoints,96)\n",
    "        p: number of runs satisfying property for each parameter, repeated (datapoints,96)\n",
    "        q: number of runs not satisfying property, repeated (datapoints,96)\n",
    "        \n",
    "    Returns:\n",
    "        Za+Zb:\n",
    "    \"\"\"\n",
    "    \n",
    "    threshold = -np.sqrt(2)*5\n",
    "    Za = []\n",
    "    y = []\n",
    "    j = 0\n",
    "    for x in X:\n",
    "        y.append([])\n",
    "        #print(x)\n",
    "        for i in x:\n",
    "            if i >= 0:\n",
    "                y[j].append(np.log(1 + erf(i/np.sqrt(2))) - np.log(2))\n",
    "            elif ((threshold < i) and (i < 0)):\n",
    "                y[j].append(np.log(1 - erf((-i)/np.sqrt(2))) - np.log(2))\n",
    "            elif i <= threshold:\n",
    "                y[j].append(-1/2 * np.log(np.pi) - np.log(2) - 1/2 * (-i) * (-i) - \\\n",
    "                np.log((-i)) + np.log(1 - 1/(-i) + 3/((-i)**4) - 15/((-i)**6) + 105/((-i)**8) - 945/((-i)**10)))\n",
    "        j+=1\n",
    "    #print(y)\n",
    "    Za = np.multiply(y, numpy.matlib.repmat(p.reshape(-1,1), 1, 96))\n",
    "\n",
    "    Zb = []\n",
    "    y = []\n",
    "    j = 0\n",
    "    for x in (-X):\n",
    "        y.append([])\n",
    "        #print(x)\n",
    "        for i in x:\n",
    "            #print(i)\n",
    "            if i >= 0:\n",
    "                y[j].append(np.log(1 + erf(i/np.sqrt(2))) - np.log(2))\n",
    "            elif ((threshold < i) and (i < 0)):\n",
    "                y[j].append(np.log(1 - erf((-i)/np.sqrt(2))) - np.log(2))\n",
    "            elif i <= threshold:\n",
    "                y[j].append(-1/2 * np.log(np.pi) - np.log(2) - 1/2 * (-i) * (-i) - \\\n",
    "                np.log((-i)) + np.log(1 - 1/(-i) + 3/((-i)**4) - 15/((-i)**6) + 105/((-i)**8) - 945/((-i)**10)))\n",
    "        j+=1\n",
    "\n",
    "    Zb = np.multiply(y, numpy.matlib.repmat(q.reshape(-1,1), 1, 96))\n",
    "    return Za + Zb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine data, such that we have everything here at one place\n",
    "\n",
    "simulation_runs = scale = 5  # number of trajectories per input point\n",
    "paramValueSet = np.linspace(0.5, 5, 20).reshape(-1,1) # uncertain input parameter that is varied = population size\n",
    "datapoints = len(paramValueSet)  # number of input points\n",
    "paramValueOutputs = np.array([1,1,0.8,1,1,1,1,0.6,0.6,0.6,0.8,0,0.8,0.2,0.6,0,0.4,0.4,0,0.2]).reshape(-1,1) # statistical outputs of satisfaction \n",
    "data = trainingSet = [paramValueSet, paramValueOutputs] # set as training set for GP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1\n",
      "Iteration  2\n",
      "Iteration  3\n",
      "Iteration  4\n",
      "Iteration  5\n",
      "Iteration  6\n",
      "Iteration  7\n",
      "Iteration  8\n",
      "Iteration  9\n",
      "Iteration  10\n",
      "Iteration  11\n",
      "Iteration  12\n",
      "Iteration  13\n",
      "Iteration  14\n",
      "Iteration  15\n",
      "Iteration  16\n",
      "Iteration  17\n",
      "Iteration  18\n",
      "Iteration  19\n",
      "Iteration  20\n",
      "Iteration  21\n",
      "Iteration  22\n",
      "Iteration  23\n",
      "Iteration  24\n",
      "Iteration  25\n",
      "Iteration  26\n",
      "Iteration  27\n"
     ]
    }
   ],
   "source": [
    "# initialization (all 0)\n",
    "# Prior\n",
    "gauss_C = kernel_rbf(paramValueSet, paramValueSet, params) + correction * np.eye(datapoints) # covariance training set\n",
    "\n",
    "gauss_LC_t = np.linalg.cholesky(gauss_C + 1e-4 * np.eye(datapoints))  # cholesky decomposition, returns A=U'*U\n",
    "gauss_LC = gauss_LC_t.T  # transpose L' (or other way round?)\n",
    "gauss_LC_diag = np.diagonal(gauss_LC).reshape(-1,1)\n",
    "\n",
    "logdet_LC = 2*np.sum(np.log(gauss_LC_diag))\n",
    "logZprior = 0.5*logdet_LC\n",
    "\n",
    "logZterms = np.zeros(datapoints).reshape(-1,1)\n",
    "logZloo = np.zeros(datapoints).reshape(-1,1)\n",
    "Term = np.zeros((datapoints, 2))  # Term[:,0] = v tilde, Term[:,1] = tau tilde\n",
    "\n",
    "# compute marginal moments mu and sigma^2\n",
    "logZappx, gauss_m, gauss_diagV = marginal_moments(Term, gauss_LC, gauss_LC_t)\n",
    "\n",
    "# related to likelihood approximation\n",
    "# true observation values (number of trajectories satisfying property)\n",
    "gauss_LikPar_p = paramValueOutputs * scale\n",
    "gauss_LikPar_q = scale - gauss_LikPar_p \n",
    "\n",
    "# gauss hermite: quadrature to approximate values of integral, returns abscissas (x) and weights (w) of\n",
    "# n-point Gauss-Hermite quadrature formula\n",
    "nodes = 96\n",
    "gauss_xGauss, gauss_wGauss = gausshermite(nodes)\n",
    "gauss_logwGauss = np.log(gauss_wGauss)\n",
    "\n",
    "# parameters for loop initialization\n",
    "MaxIter=1000\n",
    "tol=1e-6\n",
    "logZold=0\n",
    "logZ = 2*tol\n",
    "steps=0\n",
    "logZappx=0\n",
    "eps_damp=0.5\n",
    "\n",
    "while (np.abs(logZ-logZold)>tol) and (steps<MaxIter):\n",
    "#while steps==0:\n",
    "    steps += 1\n",
    "    logZold = logZ\n",
    "    \n",
    "    # find cavity distribution parameters mu_-i and sigma^2_-i\n",
    "    cav_m, cav_diagV = cavities(gauss_diagV, gauss_m, Term)\n",
    "    \n",
    "    # update marginal moments mu_hat and sigma^2_hat, and site parameters v_tilde and tau_tilde\n",
    "    Term, logZterms, logZloo = ep_update(cav_diagV, cav_m, Term, eps_damp, gauss_LikPar_p,\n",
    "                      gauss_LikPar_q, gauss_xGauss, gauss_logwGauss)\n",
    "    \n",
    "    # recompute mu and sigma^2 from the updated parameters\n",
    "    logZappx, gauss_m, gauss_diagV = marginal_moments(Term, gauss_LC, gauss_LC_t)\n",
    "    \n",
    "    logZ = logZterms.sum() + logZappx\n",
    "    \n",
    "    print(\"Iteration \", steps)\n",
    "    \n",
    "\n",
    "logZ = logZ - logZprior\n",
    "gauss_logZloo = logZloo.sum()\n",
    "gauss_logZappx = logZappx\n",
    "gauss_logZterms = logZterms\n",
    "gauss_logZ = logZ\n",
    "gauss_Term = Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Training  \n",
    "\n",
    "$\\tilde{\\mu} = 1/\\tilde{S} * \\tilde{v} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tilde = gauss_Term[:,0].reshape(-1,1)\n",
    "tau_tilde = gauss_Term[:,1].reshape(-1,1)\n",
    "\n",
    "diagSigma_tilde = 1/tau_tilde\n",
    "\n",
    "mu_tilde = np.multiply(v_tilde, diagSigma_tilde)\n",
    "\n",
    "Sigma_tilde = np.zeros((datapoints, datapoints))\n",
    "np.fill_diagonal(Sigma_tilde, diagSigma_tilde)\n",
    "\n",
    "invC = np.linalg.solve((gauss_C + Sigma_tilde), np.eye(len(mu_tilde)))\n",
    "\n",
    "#print(invC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get GP Posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test set for which posterior is derived\n",
    "#testpoints = X_s = np.linspace(0, 20, 100).reshape(-1,1)\n",
    "testpoints = X_s = np.linspace(0, 5, 100).reshape(-1,1)\n",
    "\n",
    "\n",
    "# calculate variances of testset and covariances of test & training set (apply kernel)\n",
    "kss = kernel_rbf(testpoints, testpoints, params) #+ correction * np.eye(testpoints) \n",
    "ks = kernel_rbf(testpoints, paramValueSet, params) #+ correction * np.eye(datapoints)\n",
    "\n",
    "\n",
    "# mean function mu\n",
    "fs = np.matmul(np.matmul(ks, invC), mu_tilde)\n",
    "#fs = ks.T.dot(np.linalg.solve(invC.T, np.linalg.solve(invC, mu_tilde)))\n",
    "\n",
    "# covariance function of posterior -> here only diagonal (variance)\n",
    "vfs = (np.diagonal(kss) - (np.diagonal(np.matmul(np.matmul(ks, invC), ks.T)))).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probit Regression Posterior\n",
    "Compute confidence bounds and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rf/n6c6shms3qn1gmy6qvb298k80000gn/T/ipykernel_1189/2934144140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Q: What is this for? Why added to probabilities?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcached_denominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# function to compute probit values\n",
    "def standardNormalCDF(x):\n",
    "    return 1/2 + 1/2 * erf(x * (1/np.sqrt(2)))\n",
    "\n",
    "# Q: What is this for? Why added to probabilities? \n",
    "cached_denominator = (1 / np.sqrt(1 + vfs)).reshape(-1,1)\n",
    "\n",
    "\n",
    "# get probabilities with probit function\n",
    "probabilities = standardNormalCDF(fs * cached_denominator)\n",
    "\n",
    "\n",
    "# compute confidence bounds\n",
    "lowerbound = standardNormalCDF((fs - 1.96 * np.sqrt(vfs).reshape(-1,1)) *\n",
    "                              cached_denominator)\n",
    "upperbound = standardNormalCDF((fs + 1.96 * np.sqrt(vfs).reshape(-1,1)) *\n",
    "                              cached_denominator)\n",
    "\n",
    "# plot data\n",
    "plt.plot(testpoints, probabilities, lw=1.5, ls='-')\n",
    "plt.fill_between(testpoints.ravel(), lowerbound.ravel(), upperbound.ravel(), alpha=0.2)\n",
    "plt.scatter(paramValueSet, paramValueOutputs, marker='o', c='blue')\n",
    "plt.title('Classification with 10 observations per input point')\n",
    "plt.xlabel('Population size $N$')\n",
    "plt.ylabel('$P(M_{\\theta} \\models \\varphi)$')\n",
    "#plt.yticks([0, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
